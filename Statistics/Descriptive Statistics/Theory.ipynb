{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a15944",
   "metadata": {},
   "source": [
    "# Visualization Guide for Univariate, Bivariate & Multivariate Analysis\n",
    "\n",
    "## 1. UNIVARIATE ANALYSIS (Single Variable)\n",
    "\n",
    "### For Numerical Variables\n",
    "\n",
    "| Plot | Purpose | Best When |\n",
    "|------|---------|-----------|\n",
    "| **Histogram** | Shows frequency distribution | Understanding data spread and shape |\n",
    "| **KDE Plot** | Smooth probability density curve | Visualizing continuous distribution |\n",
    "| **Box Plot** | Shows median, quartiles, outliers | Detecting outliers and spread |\n",
    "| **Violin Plot** | Box plot + density shape | Seeing full distribution shape |\n",
    "| **Rug Plot** | Individual data points as ticks | Small datasets, shows actual values |\n",
    "| **QQ Plot** | Compares to theoretical distribution | Checking normality assumption |\n",
    "\n",
    "## 2. BIVARIATE ANALYSIS (Two Variables)\n",
    "\n",
    "### NUM-NUM (Numerical vs Numerical)\n",
    "\n",
    "| Plot | Purpose | Best When |\n",
    "|------|---------|-----------|\n",
    "| **Scatter Plot** | Shows relationship between two numbers | Core relationship analysis |\n",
    "| **Regression Plot** | Scatter + fitted regression line | Checking linear relationship |\n",
    "| **Hexbin Plot** | Binned scatter (hexagonal) | Large datasets (>10k points) |\n",
    "| **2D KDE / Contour Plot** | Density of point clusters | Identifying concentration areas |\n",
    "| **Joint Plot** | Scatter + marginal distributions | Complete picture of both variables |\n",
    "| **Line Plot** | Trend over ordered variable | Time series data |\n",
    "\n",
    "### CAT-CAT (Categorical vs Categorical)\n",
    "\n",
    "| Plot | Purpose | Best When |\n",
    "|------|---------|-----------|\n",
    "| **Grouped Bar Chart** | Side-by-side comparison | Comparing counts across groups |\n",
    "| **Stacked Bar Chart** | Composition within categories | Showing part-to-whole |\n",
    "| **Heatmap** | Contingency table visualization | Many categories, frequency matrix |\n",
    "| **Mosaic Plot** | Proportional area representation | Showing relative proportions |\n",
    "\n",
    "### NUM-CAT (Numerical vs Categorical)\n",
    "\n",
    "| Plot | Purpose | Best When |\n",
    "|------|---------|-----------|\n",
    "| **Grouped Box Plot** | Distribution per category | Comparing spread across groups |\n",
    "| **Grouped Violin Plot** | Full distribution shape per category | Seeing density differences |\n",
    "| **Strip Plot** | Individual points by category | Small datasets |\n",
    "| **Swarm Plot** | Non-overlapping strip plot | Medium datasets |\n",
    "| **Bar Plot** | Mean/median with error bars | Summarizing central tendency |\n",
    "| **Point Plot** | Mean with confidence intervals | Comparing means across groups |\n",
    "| **Ridge Plot / Joy Plot** | Overlapping distributions | Many categories, beautiful viz |\n",
    "\n",
    "## 3. MULTIVARIATE ANALYSIS (3+ Variables)\n",
    "\n",
    "### Multiple Numerical Variables\n",
    "\n",
    "| Plot | Purpose | Best When |\n",
    "|------|---------|-----------|\n",
    "| **Pair Plot / Scatter Matrix** | All pairwise scatter plots | Initial exploration of relationships |\n",
    "| **Correlation Heatmap** | Visualize correlation matrix | Identifying correlated features |\n",
    "| **3D Scatter Plot** | Three numerical variables | Spatial relationship (limited use) |\n",
    "| **Bubble Chart** | Scatter + size dimension | Adding third numerical variable |\n",
    "| **Parallel Coordinates** | Multiple dimensions as parallel axes | High-dimensional comparison |\n",
    "| **Radar / Spider Chart** | Multiple metrics on radial axes | Comparing profiles |\n",
    "| **PCA / t-SNE Plot** | Dimensionality reduction visualization | Very high dimensions |\n",
    "\n",
    "### Mixed Variables (Num + Cat combined)\n",
    "\n",
    "| Plot | Purpose | Best When |\n",
    "|------|---------|-----------|\n",
    "| **Colored Scatter Plot** | Scatter with color = category | Adding categorical dimension |\n",
    "| **Facet Grid / Small Multiples** | Repeat plots for each category | Comparing patterns across groups |\n",
    "| **Pair Plot with Hue** | Scatter matrix colored by category | Multivariate + grouping |\n",
    "| **Grouped Box/Violin with Hue** | Multiple grouping levels | Two categorical + one numerical |\n",
    "\n",
    "## Quick Reference Summary Table\n",
    "\n",
    "| Analysis | Variable Types | Primary Plots |\n",
    "|----------|---------------|---------------|\n",
    "| **Univariate** | Numerical | Histogram, Box, Violin, KDE |\n",
    "| **Univariate** | Categorical | Bar/Count, Pie |\n",
    "| **Bivariate** | Num-Num | Scatter, Regression, Joint, Hexbin |\n",
    "| **Bivariate** | Cat-Cat | Grouped Bar, Stacked Bar, Heatmap |\n",
    "| **Bivariate** | Num-Cat | Box, Violin, Strip, Swarm, Bar |\n",
    "| **Multivariate** | Multiple Num | Pair Plot, Correlation Heatmap, Parallel Coords |\n",
    "| **Multivariate** | Mixed | Colored Scatter, Facet Grid, Pair Plot with Hue |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13660e57",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c78480",
   "metadata": {},
   "source": [
    "#### What is the Interquartile Range (IQR), and why is it considered a \"robust\" measure of spread compared to standard deviation?\n",
    "—→   Standard deviation is computed from the mean and squares deviations, so extreme values strongly influence it and can inflate the perceived spread. The IQR uses the median and the 25th/75th percentiles (the middle 50% of the data), so it largely ignores outliers and skewness—making it a more robust summary of typical variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd32eeb",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59de41",
   "metadata": {},
   "source": [
    "### 1. Variance vs. Covariance vs. Correlation\n",
    "\n",
    "To understand the difference, imagine you are analyzing data about people's height and weight.\n",
    "\n",
    "#### **Variance (The Spread of One)**\n",
    "Variance measures how spread out a **single** variable is from its own average.\n",
    "*   **Function:** It tells you about the dispersion of data.\n",
    "*   **Formula Concept:** Average of squared differences from the mean.\n",
    "*   **Example:** Analyzing only *Height*. High variance means you have a mix of giants and short people. Low variance means everyone is roughly the same height.\n",
    "*   **Units:** The units are squared (e.g., $\\text{cm}^2$). This makes it hard to interpret intuitively.\n",
    "\n",
    "#### **Covariance (The Joint Movement of Two)**\n",
    "Covariance measures how **two** variables change together.\n",
    "*   **Function:** It tells you the *direction* of the relationship.\n",
    "*   **Formula Concept:** Average of the product of deviations ($x$ from mean of $X$) and ($y$ from mean of $Y$).\n",
    "*   **Example:** Analyzing *Height* and *Weight*.\n",
    "    *   **Positive Covariance:** As height increases, weight tends to increase.\n",
    "    *   **Negative Covariance:** As one increases, the other decreases.\n",
    "*   **Units:** The units are the product of the two variables (e.g., $\\text{cm} \\cdot \\text{kg}$).\n",
    "\n",
    "#### **Correlation (The Standardized Relationship)**\n",
    "Correlation is the **normalized** version of covariance.\n",
    "*   **Function:** It tells you the *direction* AND the *strength* of the relationship on a standardized scale.\n",
    "*   **Formula Concept:** Covariance divided by the product of the standard deviations of $X$ and $Y$.\n",
    "*   **Units:** **Unitless.** It is a pure number between -1 and +1.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How the problem of one solves another\n",
    "\n",
    "This is a hierarchy of statistical evolution. Each concept solves a flaw in the previous one.\n",
    "\n",
    "#### **Problem 1: Variance tells us nothing about relationships.**\n",
    "Variance is excellent for understanding volatility (risk) in a single dataset, but it is \"blind\" to how that dataset interacts with the outside world.\n",
    "*   **Solution (Covariance):** By multiplying the deviations of two different variables, Covariance allows us to see if Variable A and Variable B move in sync.\n",
    "\n",
    "#### **Problem 2: Covariance has an \"Interpretation Problem\" (The Scale Issue).**\n",
    "This is the biggest issue. Because covariance keeps the units (e.g., $\\text{cm} \\cdot \\text{kg}$), the number changes based on the scale.\n",
    "*   *Scenario:* If you measure height in **meters**, you might get a covariance of **1.5**. If you convert height to **centimeters**, the relationship is exactly the same, but the covariance mathematically jumps to **150**.\n",
    "*   *The Issue:* If I tell you the covariance is 500, is that a strong relationship? You don't know. It depends on the units.\n",
    "\n",
    "#### **Solution (Correlation):**\n",
    "Correlation solves the scale problem by **dividing the covariance by the standard deviation (variance rooted)** of the variables.\n",
    "By dividing the \"co-movement\" by the \"individual spreads,\" you cancel out the units.\n",
    "*   **Result:** A correlation of 0.8 is always strong, regardless of whether you are measuring in inches, miles, or lightyears.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why are they limited to Linear Relationships?\n",
    "\n",
    "Standard Correlation (Pearson) and Covariance measure **constant rates of change**.\n",
    "\n",
    "If you look at the math: $\\sum (x - \\bar{x})(y - \\bar{y})$, it is calculating how much $Y$ deviates from the mean relative to how much $X$ deviates.\n",
    "\n",
    "**The \"Parabola\" Problem:**\n",
    "Imagine a U-shaped curve (like $y = x^2$, from $x = -10$ to $x = 10$).\n",
    "*   On the left side, as $X$ goes up (moves toward 0), $Y$ goes down. (Negative correlation).\n",
    "*   On the right side, as $X$ goes up (moves away from 0), $Y$ goes up. (Positive correlation).\n",
    "\n",
    "If you run a standard correlation calculation on this perfect mathematical relationship, the negative side cancels out the positive side. The result is **0**. Correlation claims there is \"no relationship,\" even though there is a perfect non-linear relationship.\n",
    "\n",
    "Therefore, <u>`standard covariance and correlation are only reliable when the rate of change is constant (a straight line).`</u>\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Types of Correlation Coefficients and When to Use Them\n",
    "\n",
    "Because of the linearity limitation and data types, we use different coefficients.\n",
    "\n",
    "#### **A. Pearson Correlation Coefficient ($r$)**\n",
    "*   **What is it?** The standard, most common type.\n",
    "*   **When to use:**\n",
    "    *   Both variables are continuous (e.g., height, stock price, temperature).\n",
    "    *   The relationship is **Linear**.\n",
    "    *   The data is normally distributed.\n",
    "    *   There are no extreme outliers (Pearson is very sensitive to outliers).\n",
    "\n",
    "#### **B. Spearman’s Rank Correlation ($\\rho$ or rho)**\n",
    "*   **What is it?** A non-parametric test. Instead of calculating the values, it ranks the data (1st, 2nd, 3rd highest) and calculates the correlation of the **ranks**.\n",
    "*   **When to use:**\n",
    "    *   **Non-Linear but Monotonic:** The relationship isn't a straight line, but it moves in one direction (e.g., an exponential curve).\n",
    "    *   **Ordinal Data:** Data that has an order but no fixed value (e.g., Survey results: \"Satisfied\", \"Neutral\", \"Unsatisfied\").\n",
    "    *   **Outliers:** You have extreme data points that are ruining your Pearson score. Ranking them minimizes the impact of the outlier.\n",
    "\n",
    "#### **C. Kendall’s Tau ($\\tau$)**\n",
    "*   **What is it?** Similar to Spearman but based on \"concordant and discordant pairs\" (probability math) rather than variance math.\n",
    "*   **When to use:**\n",
    "    *   **Small sample sizes.**\n",
    "    *   When there are many \"tied\" ranks (e.g., three people ranked 3rd). Kendall’s Tau is statistically more robust than Spearman in these specific cases.\n",
    "\n",
    "#### **D. Point-Biserial Correlation**\n",
    "*   **When to use:**\n",
    "    *   One variable is continuous (e.g., Test Score).\n",
    "    *   One variable is binary/dichotomous (e.g., Gender: Male/Female, or Pass/Fail)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb27511",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7fe88",
   "metadata": {},
   "source": [
    "### The Fundamental, Non-Mathematical Difference\n",
    "\n",
    "**Correlation** describes a statistical relationship where two variables tend to change together in a predictable way. It's an observation about a pattern. You can think of it as **\"What\"** — *what* is happening in the data.\n",
    "\n",
    "**Causation** implies that a change in one variable is directly responsible for producing a change in the other. It's about a mechanism of action or influence. You can think of it as **\"Why\"** — *why* it is happening.\n",
    "\n",
    "In essence:\n",
    "*   **Correlation is about association.** (X and Y move together).\n",
    "*   **Causation is about consequence.** (X *makes* Y happen).\n",
    "\n",
    "---\n",
    "\n",
    "### Why a Strong Correlation Can Be Completely Misleading\n",
    "\n",
    "A strong correlation can be a red flag waving at an interesting relationship, but it doesn't tell you *why* that relationship exists. It can be misleading because the observed link can be explained by other, hidden factors. Here are the primary reasons:\n",
    "\n",
    "1.  **The Third Variable (Confounding/Lurking Variable):** This is the most common culprit. A third, unmeasured variable is causing *both* of the observed variables to change.\n",
    "    *   **Classic Example:** There is a strong positive correlation between *ice cream sales* and *swimming pool drownings*. Does buying ice cream cause drowning? No. The lurking variable is **hot weather (summer season)**. Hot weather causes more people to buy ice cream *and* more people to swim, leading to more drownings.\n",
    "\n",
    "2.  **Coincidence (Spurious Correlation):** Sometimes two variables trend together purely by random chance, with no logical connection at all.\n",
    "    *   **Example:** There is a historically strong correlation between the number of films Nicolas Cage appeared in a given year and the number of people who drowned by falling into a pool. The link is utterly coincidental and meaningless.\n",
    "\n",
    "3.  **Reverse Causation:** The direction of cause and effect is the opposite of what one might assume. Correlation doesn't tell you which variable is the cause and which is the effect.\n",
    "    *   **Example:** A study finds a correlation between low self-esteem and depression. Does low self-esteem *cause* depression? Possibly. But it is equally plausible that being depressed *causes* low self-esteem. The correlation alone cannot untangle this.\n",
    "\n",
    "4.  **Selection Bias (The Sample is Not Representative):** The observed correlation exists only within a specific, non-random sample and does not reflect a true relationship in the broader population.\n",
    "    *   **Example:** A university finds a strong correlation between students' ownership of textbooks and their final grades. It would be misleading to conclude textbooks *cause* better grades. The sample is biased—it only includes students who enrolled and stayed in the course. It excludes students who dropped out early (who might have had textbooks but poor grades). The relationship might be driven by a student's overall motivation or commitment.\n",
    "\n",
    "**Key Takeaway:** A strong correlation is a clue, not a conclusion. It signals that something interesting *might* be happening and warrants deeper investigation. Establishing causation requires logical reasoning, controlled experiments (where possible), and ruling out these alternative explanations—it moves from simply observing a pattern to understanding the underlying process.\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Problem in ML\n",
    "Most ML models are **masters of correlation, not causation**. They find patterns (correlations) in historical data to make predictions. They assume the future will behave like the past. But if those patterns are not causal, the model can fail spectacularly when you try to use it to make decisions that *change* the world.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Affects ML in Practice: Simple Examples\n",
    "\n",
    "#### 1. **Models Learn to Predict, Not to Understand \"Why\"**\n",
    "*   **Example:** A bank builds a model to predict loan defaults. It finds a strong correlation: **\"People who buy high-quality printers are less likely to default.\"**\n",
    "*   **The Correlation Trap:** The model might use \"printer ownership\" as a key signal for a safe borrower.\n",
    "*   **The Causal Reality:** Buying an expensive printer doesn't *cause* financial stability. Both are likely caused by a **lurking variable**: being **organized and financially responsible**. A responsible person buys a good printer *and* pays their bills.\n",
    "*   **The Harm:** If the bank now **denies loans to people without good printers**, it's committing discrimination based on a spurious correlation. It's rejecting potentially good customers for the wrong reason.\n",
    "\n",
    "#### 2. **Models Break When the World Changes (Lack of Robustness)**\n",
    "*   **Example:** An e-commerce ML model learns to recommend products. It finds a strong historical correlation: **\"People who click on article links about 'sunscreen' also buy swimsuits.\"** So, it recommends swimsuits on sunscreen pages.\n",
    "*   **The Correlation Trap:** The model assumes this link is fixed.\n",
    "*   **The Causal Reality:** The link wasn't causal; it was driven by context (**summer season**). In summer, people read about sun protection *and* buy beachwear.\n",
    "*   **The Harm:** If you deploy this model in **December**, the recommendations fail. The model didn't understand the *cause* (summer intent), it just memorized a seasonal correlation. A causal model would understand the deeper \"beach trip\" intent.\n",
    "\n",
    "#### 3. **Models Can Reinforce Bias and Create Feedback Loops**\n",
    "*   **Example:** A police department uses an ML model to predict crime hotspots based on historical arrest data.\n",
    "*   **The Correlation Trap:** The model finds a \"high crime\" correlation with certain neighborhoods and recommends sending more police there.\n",
    "*   **The Causal Reality:** The historical data might reflect **biased policing patterns** (more police in certain areas lead to more arrests, not necessarily more crime). The correlation is between \"police presence\" and \"reported crime,\" not underlying criminal activity.\n",
    "*   **The Harm:** The model sends even more police to that neighborhood, leading to even more arrests, which the model sees as \"confirming\" its prediction. This creates a **toxic feedback loop** that amplifies historical bias, all because it mistook a correlation (arrests) for the cause of crime.\n",
    "\n",
    "#### 4. **Models Give Bad Advice for Interventions**\n",
    "*   **This is the biggest practical consequence.** ML is often used to answer \"What should I do?\"\n",
    "*   **Correlation-Based Answer:** \"What is likely to happen next?\" (Prediction)\n",
    "*   **Causation-Based Answer:** \"What will happen **if I do X**?\" (Intervention)\n",
    "*   **Simple Example:**\n",
    "    *   **Data shows a correlation:** Hospital patients with a dedicated family visitor (Variable A) recover faster (Variable B).\n",
    "    *   **A correlation-only model suggests:** To speed recovery, assign a family visitor.\n",
    "    *   **The causal truth:** The family visitor isn't the *cause* of recovery. The **true cause** is the patient's **underlying will to live and strength**. That same strength also motivates family to visit. Forcing a visitor on a lonely patient won't cause the same effect. The model's recommendation is useless or wasteful.\n",
    "\n",
    "### The Simple Analogy\n",
    "Think of a machine learning model as a **brilliant pattern-spotting detective**.\n",
    "*   It sees that **every time the rooster crows (A), the sun rises (B)**. It learns this pattern perfectly.\n",
    "*   If you ask it, **\"Will the sun rise tomorrow?\"** it will correctly predict **\"Yes, if the rooster crows.\"** (This is **prediction** using correlation).\n",
    "*   But if you ask it, **\"How can I make the sun rise earlier?\"** and it answers **\"Make the rooster crow earlier,\"** it is catastrophically wrong. It confused correlation (B follows A) with causation (A causes B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d94312",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
